{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c980ebae",
   "metadata": {},
   "source": [
    "# Cybersecurity Web Threats — Isolation Forest Baseline (multi-cell)\n",
    "_Generated: 2025-11-08 10:36_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca769db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================== ALL-IN-ONE FINAL CELL ===============================\n",
    "# Robust Isolation Forest baseline with leakage-safe split, train-based thresholding,\n",
    "# label-aware evaluation (with single-class test fallback), and Top-N anomalies table.\n",
    "\n",
    "# --- Imports & global config\n",
    "import os, math, warnings, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "TARGET_RATE = 0.0532  # 5.32% target anomaly rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Data loading (robust)\n",
    "DATA_PATH = \"/content/processed_web_threats.csv\"           # primary (Colab)\n",
    "FALLBACK_PATH = \"/mnt/data/processed_web_threats.csv\"      # secondary (this chat)\n",
    "def read_any(path: str):\n",
    "    p = str(path)\n",
    "    if p.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(p)\n",
    "    if p.lower().endswith(\".parquet\"):\n",
    "        return pd.read_parquet(p)\n",
    "    try:\n",
    "        return pd.read_parquet(p)\n",
    "    except Exception:\n",
    "        return pd.read_csv(p)\n",
    "\n",
    "def generate_synthetic(n=6000, start=\"2025-01-04 08:00:00\"):\n",
    "    rng = np.random.default_rng(123)\n",
    "    ts = pd.date_range(start=start, periods=n, freq=\"min\")\n",
    "    bin_ = np.clip(rng.normal(2500, 600, n), 200, 8000)\n",
    "    bout = np.clip(rng.normal(4200, 900, n), 200, 16000)\n",
    "    dur = np.full(n, 60.0)\n",
    "    status = rng.choice([200, 302, 403, 404, 500], p=[0.7,0.1,0.05,0.1,0.05], size=n)\n",
    "    sess = rng.integers(1e6, 9e6, size=n)\n",
    "    src_ip = rng.choice([f\"10.0.0.{i}\" for i in range(1,255)], size=n)\n",
    "    dst_ip = rng.choice([f\"172.16.0.{i}\" for i in range(1,255)], size=n)\n",
    "    req_count = rng.poisson(5, size=n).astype(float)\n",
    "    anomaly = ((bin_+bout)/dur > 118) | (req_count > 12)\n",
    "    df_syn = pd.DataFrame({\n",
    "        \"time\": ts.astype(str),\n",
    "        \"session_id\": sess,\n",
    "        \"src_ip\": src_ip,\n",
    "        \"dst_ip\": dst_ip,\n",
    "        \"status\": status,\n",
    "        \"bytes_in\": bin_,\n",
    "        \"bytes_out\": bout,\n",
    "        \"session_duration_s\": dur,\n",
    "        \"req_count\": req_count,\n",
    "        \"is_waf_rule\": anomaly.astype(int),\n",
    "    })\n",
    "    return df_syn\n",
    "\n",
    "if DATA_PATH and Path(DATA_PATH).exists():\n",
    "    df = read_any(DATA_PATH)\n",
    "    print(f\"Loaded data from {DATA_PATH} | rows={len(df):,}\")\n",
    "elif Path(FALLBACK_PATH).exists():\n",
    "    df = read_any(FALLBACK_PATH)\n",
    "    print(f\"Loaded data from {FALLBACK_PATH} | rows={len(df):,}\")\n",
    "else:\n",
    "    df = generate_synthetic()\n",
    "    print(f\"Synthetic data generated | rows={len(df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2) Feature engineering (safe & flexible)\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Time column detection & parsing\n",
    "time_candidates = [c for c in [\"timestamp\",\"time\",\"creation_time\",\"end_time\"] if c in df_fe.columns]\n",
    "time_col = time_candidates[0] if time_candidates else None\n",
    "if time_col is None:\n",
    "    df_fe[\"timestamp\"] = pd.date_range(\"2025-01-01\", periods=len(df_fe), freq=\"min\")\n",
    "    time_col = \"timestamp\"\n",
    "df_fe[time_col] = pd.to_datetime(df_fe[time_col], errors=\"coerce\")\n",
    "\n",
    "# Duration seconds\n",
    "if \"session_duration_s\" in df_fe.columns:\n",
    "    duration_s = pd.to_numeric(df_fe[\"session_duration_s\"], errors=\"coerce\").fillna(60.0)\n",
    "else:\n",
    "    # fallback from creation/end if present\n",
    "    if \"creation_time\" in df_fe.columns and \"end_time\" in df_fe.columns:\n",
    "        ct = pd.to_datetime(df_fe[\"creation_time\"], errors=\"coerce\")\n",
    "        et = pd.to_datetime(df_fe[\"end_time\"], errors=\"coerce\")\n",
    "        duration_s = (et - ct).dt.total_seconds().fillna(60.0).replace(0, 60.0)\n",
    "    else:\n",
    "        duration_s = pd.Series([60.0]*len(df_fe), index=df_fe.index)\n",
    "df_fe[\"duration_s\"] = duration_s\n",
    "\n",
    "# Numeric basics & safe conversions\n",
    "df_fe[\"bytes_in\"]   = pd.to_numeric(df_fe[\"bytes_in\"]  if \"bytes_in\"  in df_fe.columns else 0.0, errors=\"coerce\")\n",
    "df_fe[\"bytes_out\"]  = pd.to_numeric(df_fe[\"bytes_out\"] if \"bytes_out\" in df_fe.columns else 0.0, errors=\"coerce\")\n",
    "df_fe[\"bytes_in\"]   = df_fe[\"bytes_in\"].fillna(0.0)\n",
    "df_fe[\"bytes_out\"]  = df_fe[\"bytes_out\"].fillna(0.0)\n",
    "\n",
    "if \"total_bytes\" in df_fe.columns:\n",
    "    df_fe[\"total_bytes\"] = pd.to_numeric(df_fe[\"total_bytes\"], errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    df_fe[\"total_bytes\"] = (df_fe[\"bytes_in\"] + df_fe[\"bytes_out\"]).astype(float)\n",
    "\n",
    "if \"req_count\" in df_fe.columns:\n",
    "    df_fe[\"req_count\"] = pd.to_numeric(df_fe[\"req_count\"], errors=\"coerce\").fillna(1.0)\n",
    "else:\n",
    "    df_fe[\"req_count\"] = 1.0\n",
    "\n",
    "# status from response.code if needed\n",
    "if \"status\" in df_fe.columns:\n",
    "    df_fe[\"status\"] = pd.to_numeric(df_fe[\"status\"], errors=\"coerce\").fillna(200).astype(int)\n",
    "elif \"response.code\" in df_fe.columns:\n",
    "    df_fe[\"status\"] = pd.to_numeric(df_fe[\"response.code\"], errors=\"coerce\").fillna(200).astype(int)\n",
    "else:\n",
    "    df_fe[\"status\"] = 200\n",
    "\n",
    "# Derived rates\n",
    "eps = 1e-6\n",
    "df_fe[\"bytes_per_sec\"] = df_fe[\"total_bytes\"] / df_fe[\"duration_s\"].replace(0, eps)\n",
    "df_fe[\"req_per_min\"]   = df_fe[\"req_count\"] / (df_fe[\"duration_s\"].replace(0, eps)/60.0)\n",
    "\n",
    "# Prefer provided engineered rate features if present; else compute\n",
    "if \"in_rate_bps\" not in df_fe.columns:\n",
    "    df_fe[\"in_rate_bps\"] = df_fe[\"bytes_in\"] / df_fe[\"duration_s\"].replace(0, eps)\n",
    "if \"out_rate_bps\" not in df_fe.columns:\n",
    "    df_fe[\"out_rate_bps\"] = df_fe[\"bytes_out\"] / df_fe[\"duration_s\"].replace(0, eps)\n",
    "if \"total_rate_bps\" not in df_fe.columns:\n",
    "    df_fe[\"total_rate_bps\"] = df_fe[\"total_bytes\"] / df_fe[\"duration_s\"].replace(0, eps)\n",
    "\n",
    "# Extras if present\n",
    "if \"bytes_diff\" not in df_fe.columns and {\"bytes_in\",\"bytes_out\"}.issubset(df_fe.columns):\n",
    "    df_fe[\"bytes_diff\"] = df_fe[\"bytes_out\"] - df_fe[\"bytes_in\"]\n",
    "if \"in_out_ratio\" not in df_fe.columns and {\"bytes_in\",\"bytes_out\"}.issubset(df_fe.columns):\n",
    "    df_fe[\"in_out_ratio\"] = (df_fe[\"bytes_in\"] + eps) / (df_fe[\"bytes_out\"] + eps)\n",
    "\n",
    "# Simple time/error features\n",
    "df_fe[\"hour\"] = df_fe[time_col].dt.hour\n",
    "df_fe[\"is_error\"] = (df_fe[\"status\"] >= 400).astype(int)\n",
    "\n",
    "# Weak label mapping (if available)\n",
    "label_col = None\n",
    "if \"is_waf_rule\" in df_fe.columns:\n",
    "    label_col = \"is_waf_rule\"\n",
    "elif \"detection_types\" in df_fe.columns:\n",
    "    df_fe[\"is_waf_rule\"] = df_fe[\"detection_types\"].astype(str).str.lower().str.contains(\"waf\").astype(int)\n",
    "    label_col = \"is_waf_rule\"\n",
    "\n",
    "# Final feature set (ordered by usefulness)\n",
    "feature_priority = [\n",
    "    \"in_rate_bps\",\"out_rate_bps\",\"total_rate_bps\",\n",
    "    \"total_bytes\",\"bytes_in\",\"bytes_out\",\n",
    "    \"duration_s\",\"bytes_per_sec\",\"req_per_min\",\n",
    "    \"bytes_diff\",\"in_out_ratio\",\"hour\",\"is_error\"\n",
    "]\n",
    "feature_cols = [c for c in feature_priority if c in df_fe.columns]\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"No valid feature columns found. Verify your dataframe columns.\")\n",
    "\n",
    "print(\"✅ time_col:\", time_col)\n",
    "print(\"✅ label_col:\", label_col)\n",
    "print(\"✅ feature_cols:\", feature_cols[:10], \"...\" if len(feature_cols) > 10 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) Quick EDA (safe; skip if columns missing)\n",
    "try:\n",
    "    for c in [\"duration_s\",\"bytes_in\",\"bytes_out\",\"total_bytes\",\"total_rate_bps\",\"in_rate_bps\",\"out_rate_bps\"]:\n",
    "        if c in df_fe.columns:\n",
    "            df_fe[c].dropna().plot.hist(bins=50)\n",
    "            plt.title(f\"Distribution: {c}\")\n",
    "            plt.xlabel(c); plt.ylabel(\"count\"); plt.show()\n",
    "    if \"total_rate_bps\" in df_fe.columns:\n",
    "        df_fe.set_index(time_col)[\"total_rate_bps\"].plot()\n",
    "        plt.title(\"Throughput over time\"); plt.xlabel(\"time\"); plt.ylabel(\"total_rate_bps\"); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[EDA skipped due to plotting error]\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4) Split + Pipeline + Threshold (train-based) + Evaluation\n",
    "df_fe = df_fe.sort_values(time_col).reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df_fe))\n",
    "df_train, df_test = df_fe.iloc[:split_idx].copy(), df_fe.iloc[split_idx:].copy()\n",
    "\n",
    "X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "X_test  = df_test[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))),\n",
    "    (\"iforest\", IsolationForest(\n",
    "        n_estimators=300,\n",
    "        max_samples=\"auto\",\n",
    "        contamination=TARGET_RATE,\n",
    "        max_features=1.0,\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "        warm_start=False\n",
    "    ))\n",
    "])\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# Calibrate threshold on TRAIN (no test leakage)\n",
    "scores_train = pipe.named_steps[\"iforest\"].decision_function(pipe.named_steps[\"scaler\"].transform(X_train))\n",
    "thr = np.quantile(scores_train, TARGET_RATE)\n",
    "\n",
    "scores_test = pipe.named_steps[\"iforest\"].decision_function(pipe.named_steps[\"scaler\"].transform(X_test))\n",
    "df_test[\"iso_anomaly\"] = (scores_test <= thr).astype(int)\n",
    "\n",
    "print(f\"Train anomaly rate (raw IF): {(pipe.named_steps['iforest'].predict(pipe.named_steps['scaler'].transform(X_train)) == -1).mean():.3%}\")\n",
    "print(f\"Test anomaly rate (calibrated): {df_test['iso_anomaly'].mean():.3%}\")\n",
    "\n",
    "def evaluate(df_train, df_test, scores_test, label_col):\n",
    "    if label_col and (label_col in df_test.columns):\n",
    "        # If test is single-class but train has both classes, redo split with stratify for FAIR metrics\n",
    "        if df_test[label_col].nunique() < 2 and df_train[label_col].nunique() > 1:\n",
    "            print(\"⚠️ Test set single-class; redoing split with stratify for fair metrics.\")\n",
    "            dtrain, dtest = train_test_split(\n",
    "                df_fe, test_size=0.2, random_state=SEED,\n",
    "                stratify=df_fe[label_col] if df_fe[label_col].nunique() > 1 else None\n",
    "            )\n",
    "            Xtr = dtrain[feature_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "            Xte = dtest[feature_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "            pipe.fit(Xtr)\n",
    "            s_tr = pipe.named_steps[\"iforest\"].decision_function(pipe.named_steps[\"scaler\"].transform(Xtr))\n",
    "            thr2 = np.quantile(s_tr, TARGET_RATE)\n",
    "\n",
    "            s_te = pipe.named_steps[\"iforest\"].decision_function(pipe.named_steps[\"scaler\"].transform(Xte))\n",
    "            dtest[\"iso_anomaly\"] = (s_te <= thr2).astype(int)\n",
    "\n",
    "            print(\"Confusion matrix:\\n\", confusion_matrix(dtest[label_col], dtest[\"iso_anomaly\"]))\n",
    "            print(\"\\nClassification report:\\n\", classification_report(dtest[label_col], dtest[\"iso_anomaly\"], digits=3))\n",
    "            # Invert score (lower = more anomalous) for PR/ROC\n",
    "            ap = average_precision_score(dtest[label_col], -s_te)\n",
    "            try:\n",
    "                roc = roc_auc_score(dtest[label_col], -s_te)\n",
    "            except ValueError:\n",
    "                roc = None\n",
    "            print(f\"Avg Precision (PR-AUC): {ap:.3f} | ROC-AUC: {roc if roc is not None else 'NA'}\")\n",
    "        else:\n",
    "            y_true = df_test[label_col].astype(int).values\n",
    "            y_pred = df_test[\"iso_anomaly\"].astype(int).values\n",
    "            print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "            print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, digits=3))\n",
    "            ap = average_precision_score(y_true, -scores_test)\n",
    "            try:\n",
    "                roc = roc_auc_score(y_true, -scores_test)\n",
    "            except ValueError:\n",
    "                roc = None\n",
    "            print(f\"Avg Precision (PR-AUC): {ap:.3f} | ROC-AUC: {roc if roc is not None else 'NA'}\")\n",
    "    else:\n",
    "        print(\"No labels available or dataset single-class; reported anomaly rate only.\")\n",
    "\n",
    "evaluate(df_train, df_test, scores_test, label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5) Top-N suspicious anomalies (for SOC triage) — robust columns\n",
    "cols = []\n",
    "for c in [\"timestamp\",\"time\",\"creation_time\",\"end_time\"]:\n",
    "    if c in df_fe.columns:\n",
    "        cols.append(c); break\n",
    "cols += [c for c in [\"src_ip\",\"dst_ip\",\"protocol\",\"bytes_in\",\"bytes_out\",\n",
    "                     \"total_rate_bps\",\"duration_s\",\"in_out_ratio\",\"is_waf_rule\"]\n",
    "         if c in df_fe.columns]\n",
    "\n",
    "anom_idx = df_test.index[df_test[\"iso_anomaly\"] == 1]\n",
    "df_top = df_fe.loc[anom_idx].copy()\n",
    "sort_key = \"total_rate_bps\" if \"total_rate_bps\" in df_top.columns else (\"bytes_per_sec\" if \"bytes_per_sec\" in df_top.columns else None)\n",
    "if sort_key:\n",
    "    df_top = df_top.sort_values(sort_key, ascending=False)\n",
    "\n",
    "N = 15\n",
    "display_cols = [c for c in cols if c in df_top.columns]\n",
    "df_top_anomalies = df_top[display_cols].head(N)\n",
    "print(\"\\nTop anomalies (first 15 rows):\")\n",
    "display(df_top_anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0208a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6) Optional: quick scoring function for future batches\n",
    "def score_batch(df_new: pd.DataFrame, rate=TARGET_RATE):\n",
    "    d = df_new.copy()\n",
    "\n",
    "    # Detect/parse time col\n",
    "    time_candidates = [c for c in [\"timestamp\",\"time\",\"creation_time\",\"end_time\"] if c in d.columns]\n",
    "    tcol = time_candidates[0] if time_candidates else None\n",
    "    if tcol is not None:\n",
    "        d[tcol] = pd.to_datetime(d[tcol], errors=\"coerce\")\n",
    "\n",
    "    # Ensure minimal features\n",
    "    if \"bytes_in\" not in d.columns: d[\"bytes_in\"] = 0.0\n",
    "    if \"bytes_out\" not in d.columns: d[\"bytes_out\"] = 0.0\n",
    "\n",
    "    if \"duration_s\" not in d.columns:\n",
    "        if \"session_duration_s\" in d.columns:\n",
    "            d[\"duration_s\"] = pd.to_numeric(d[\"session_duration_s\"], errors=\"coerce\").fillna(60.0)\n",
    "        else:\n",
    "            d[\"duration_s\"] = 60.0\n",
    "\n",
    "    if \"total_bytes\" not in d.columns:\n",
    "        d[\"total_bytes\"] = (pd.to_numeric(d[\"bytes_in\"], errors=\"coerce\").fillna(0.0) +\n",
    "                            pd.to_numeric(d[\"bytes_out\"], errors=\"coerce\").fillna(0.0))\n",
    "\n",
    "    eps = 1e-6\n",
    "    if \"in_rate_bps\"  not in d.columns: d[\"in_rate_bps\"]  = d[\"bytes_in\"]  / d[\"duration_s\"].replace(0, eps)\n",
    "    if \"out_rate_bps\" not in d.columns: d[\"out_rate_bps\"] = d[\"bytes_out\"] / d[\"duration_s\"].replace(0, eps)\n",
    "    if \"total_rate_bps\" not in d.columns: d[\"total_rate_bps\"] = d[\"total_bytes\"] / d[\"duration_s\"].replace(0, eps)\n",
    "    if \"bytes_per_sec\"  not in d.columns: d[\"bytes_per_sec\"]  = d[\"total_bytes\"] / d[\"duration_s\"].replace(0, eps)\n",
    "    if \"req_count\" not in d.columns: d[\"req_count\"] = 1.0\n",
    "    if \"req_per_min\" not in d.columns: d[\"req_per_min\"] = d[\"req_count\"] / (d[\"duration_s\"].replace(0, eps)/60.0)\n",
    "    if \"status\" not in d.columns:\n",
    "        d[\"status\"] = 200\n",
    "    d[\"is_error\"] = (pd.to_numeric(d[\"status\"], errors=\"coerce\").fillna(200).astype(int) >= 400).astype(int)\n",
    "    d[\"hour\"] = pd.to_datetime(d[tcol], errors=\"coerce\").dt.hour if tcol else 0\n",
    "\n",
    "    feats = [c for c in [\"in_rate_bps\",\"out_rate_bps\",\"total_rate_bps\",\"total_bytes\",\"bytes_in\",\"bytes_out\",\n",
    "                         \"duration_s\",\"bytes_per_sec\",\"req_per_min\",\"bytes_diff\",\"in_out_ratio\",\"hour\",\"is_error\"]\n",
    "             if c in d.columns]\n",
    "\n",
    "    X = d[feats].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    s = pipe.named_steps[\"iforest\"].decision_function(pipe.named_steps[\"scaler\"].transform(X))\n",
    "    thr_local = np.quantile(s, rate)  # local rate; for prod use global thr if desired\n",
    "    out = pd.DataFrame({\n",
    "        \"id\": d.get(\"session_id\", pd.Series(range(len(d)))),\n",
    "        \"timestamp\": d.get(tcol, pd.Series([None]*len(d))),\n",
    "        \"score\": s,\n",
    "        \"is_anomaly\": (s <= thr_local).astype(int),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "print(\"\\n✅ Finished. Variables available: df_fe, df_train, df_test, feature_cols, pipe, thr, df_top_anomalies\")\n",
    "# =============================== END ALL-IN-ONE CELL ===============================\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
